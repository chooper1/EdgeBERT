{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entropypredictor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9vEf0rECsft"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from matplotlib.pyplot import plot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpogxwFXDTsd"
      },
      "source": [
        "#load dataset\n",
        "fn2 = \"log_test_MNLI_entropy_opt_12class.txt\"\n",
        "fn = \"log_train_MNLI_entropy_opt_12class.txt\"\n",
        "\n",
        "nparr2 = np.loadtxt(fn2, delimiter=\",\")\n",
        "nparr = np.loadtxt(fn, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcTwqf21OOfY"
      },
      "source": [
        "#split 80/20\n",
        "#whether to average entropies up to start_layer or just use entropy at start layer\n",
        "avg = 0 \n",
        "#layer after which you make entropy prediction\n",
        "start_layer = 6 #indexed 0-11\n",
        "\n",
        "#split = round(0.95*nparr.shape[0])\n",
        "#train = nparr[:split,:]\n",
        "#test = nparr[split:,:]\n",
        "test = nparr2[:,:11]\n",
        "train = nparr[:,:11]\n",
        "\n",
        "if avg:\n",
        "  (x_train, y_train) = (train[:,:start_layer], train[:,start_layer+1:])\n",
        "  (x_test, y_test) = (test[:,:start_layer], test[:,start_layer+1:])\n",
        "  x_train = np.mean(x_train, axis=-1)\n",
        "  x_test = np.mean(x_test, axis=-1)\n",
        "else:\n",
        "  (x_train, y_train) = (train[:,start_layer], train[:,start_layer+1:])\n",
        "  (x_test, y_test) = (test[:,start_layer], test[:,start_layer+1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxnIly95IPWm",
        "outputId": "6b597dad-197c-43cf-aee3-ffb059399b7c"
      },
      "source": [
        "# E1 -> E2 E3 E4 E5 E6 E7 E8 E9 E10 E11\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_dim = 1)) #input layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "#model.add(Dense(64, activation='linear'))\n",
        "model.add(Dense(10-start_layer)) # output layer\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mean_squared_error' \n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, epochs=3, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12268/12268 [==============================] - 19s 2ms/step - loss: 0.0497\n",
            "Epoch 2/3\n",
            "12268/12268 [==============================] - 19s 2ms/step - loss: 0.0484\n",
            "Epoch 3/3\n",
            "12268/12268 [==============================] - 19s 2ms/step - loss: 0.0483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdcb8415850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ylyMrT3rx6o",
        "outputId": "f992994a-5180-4cda-9347-91e7df118579"
      },
      "source": [
        "#check prediction\n",
        "\n",
        "val_loss = model.evaluate(x_test, y_test)\n",
        "#val_loss = model.evaluate(x_train,y_train)\n",
        "#model.summary()\n",
        "\n",
        "\n",
        "#i=7\n",
        "#x = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "#while(i < 10):\n",
        "#inp = x_test[i]\n",
        "#out = y_test[i]\n",
        "#pred = model.predict(np.array([inp]), batch_size=1)\n",
        "#print(\"prediction:\")\n",
        "#print(pred[0])\n",
        "#print(\"output:\")\n",
        "#print(out)\n",
        "#i = i + 1\n",
        "#print(inp)\n",
        "#plot(x, out)\n",
        "#plot(x, np.transpose(pred))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "307/307 [==============================] - 0s 924us/step - loss: 0.0505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s-qMGKPzywb",
        "outputId": "1b0e08d7-9af3-4d0d-9d86-dca22da7a0b9"
      },
      "source": [
        "#produce lookup table\n",
        "table = np.zeros((100, 11-start_layer))\n",
        "for i in range(0,100):\n",
        "  xval = i*0.01;\n",
        "  yval = model.predict(np.array([xval]), batch_size=1)\n",
        "  table[i,0] = xval\n",
        "  table[i,1:] = yval[0]\n",
        "\n",
        "threshold_indices = table < 0\n",
        "table[threshold_indices] = 0\n",
        "\n",
        "np.savetxt(\"mnli_lookup_table_opt_layer7.csv\", table, delimiter=\",\")\n",
        "print(table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.02224375 0.0159677  0.01438424 0.01826167]\n",
            " [0.01       0.02224375 0.0159677  0.01438424 0.01826167]\n",
            " [0.02       0.02224375 0.0159677  0.01438424 0.01826167]\n",
            " [0.03       0.02238394 0.01607253 0.01447886 0.01834077]\n",
            " [0.04       0.02760141 0.01997368 0.01799999 0.02128394]\n",
            " [0.05       0.03281888 0.02387486 0.02152111 0.02422711]\n",
            " [0.06       0.03936858 0.02877179 0.02593736 0.0279176 ]\n",
            " [0.07       0.04595281 0.03370483 0.03049038 0.03174715]\n",
            " [0.08       0.05253609 0.03865106 0.03519636 0.035738  ]\n",
            " [0.09       0.0591194  0.04359733 0.03990234 0.03972886]\n",
            " [0.1        0.06570272 0.04854359 0.04460832 0.04371972]\n",
            " [0.11       0.07228601 0.05348983 0.04931431 0.04771059]\n",
            " [0.12       0.0788693  0.05843608 0.05402029 0.05170144]\n",
            " [0.13       0.08535373 0.06331113 0.05868969 0.05566829]\n",
            " [0.14       0.09333654 0.06896409 0.06450999 0.06065941]\n",
            " [0.15       0.10160741 0.07477289 0.07053909 0.06583366]\n",
            " [0.16       0.10971011 0.08039175 0.075577   0.06998681]\n",
            " [0.17       0.11733764 0.0855835  0.07912354 0.07263527]\n",
            " [0.18       0.12496517 0.09077524 0.08267008 0.07528373]\n",
            " [0.19       0.13259271 0.09596701 0.08621663 0.07793219]\n",
            " [0.2        0.14022022 0.10115875 0.08976316 0.08058065]\n",
            " [0.21       0.14784776 0.1063505  0.0933097  0.08322912]\n",
            " [0.22       0.15534911 0.11145599 0.09679828 0.08583455]\n",
            " [0.23       0.16185734 0.11588252 0.09983063 0.08810131]\n",
            " [0.24       0.1683656  0.12030907 0.102863   0.09036807]\n",
            " [0.25       0.17487384 0.12473562 0.10589537 0.09263483]\n",
            " [0.26       0.18138207 0.12916216 0.10892773 0.09490159]\n",
            " [0.27       0.18930361 0.13478138 0.11327737 0.09770476]\n",
            " [0.28       0.19817702 0.14111987 0.11827454 0.10085291]\n",
            " [0.29       0.20749292 0.14777471 0.12351295 0.10415392]\n",
            " [0.3        0.21472457 0.15294109 0.12756684 0.1067099 ]\n",
            " [0.31       0.22195619 0.15810743 0.13162073 0.10926588]\n",
            " [0.32       0.22918782 0.1632738  0.13567463 0.11182187]\n",
            " [0.33       0.23641947 0.16844016 0.13972853 0.11437785]\n",
            " [0.34       0.24368246 0.17362921 0.14379983 0.1169445 ]\n",
            " [0.35       0.25177225 0.17941637 0.14832991 0.11979242]\n",
            " [0.36       0.26033264 0.18571159 0.153128   0.12279364]\n",
            " [0.37       0.27106524 0.19392428 0.15865166 0.12685288]\n",
            " [0.38       0.27960917 0.20061105 0.16323805 0.13001688]\n",
            " [0.39       0.28811675 0.20727067 0.16780621 0.13316715]\n",
            " [0.4        0.29523939 0.21289529 0.17167987 0.13579464]\n",
            " [0.41       0.3038682  0.21971227 0.17637068 0.13897702]\n",
            " [0.42       0.30863616 0.22352025 0.17891446 0.14072022]\n",
            " [0.43       0.31367469 0.22753283 0.18157017 0.14258093]\n",
            " [0.44       0.32237324 0.23423859 0.18617944 0.14595363]\n",
            " [0.45       0.33177513 0.24146695 0.19116017 0.14961202]\n",
            " [0.46       0.34117702 0.24869534 0.19614089 0.15327042]\n",
            " [0.47       0.35111266 0.25634071 0.20142734 0.1571568 ]\n",
            " [0.48       0.35782266 0.26168367 0.20554    0.16026796]\n",
            " [0.49       0.36516762 0.26753503 0.21005009 0.16367203]\n",
            " [0.5        0.37332457 0.27405971 0.21515784 0.16743787]\n",
            " [0.51       0.38306481 0.28174034 0.22029772 0.17247579]\n",
            " [0.52       0.39377213 0.2901243  0.22571895 0.17814194]\n",
            " [0.53       0.40508753 0.2988686  0.23169705 0.18327776]\n",
            " [0.54       0.41779637 0.30843854 0.23895103 0.18719864]\n",
            " [0.55       0.43156752 0.31880927 0.24681228 0.19145334]\n",
            " [0.56       0.44882107 0.33302206 0.25708055 0.19685771]\n",
            " [0.57       0.46034074 0.34261635 0.26400274 0.20084023]\n",
            " [0.58       0.47028697 0.35099506 0.26992005 0.20469564]\n",
            " [0.59       0.48023325 0.35937375 0.27583733 0.20855106]\n",
            " [0.6        0.49017954 0.36775249 0.28175467 0.21240652]\n",
            " [0.61       0.50012577 0.37613118 0.28767192 0.21626192]\n",
            " [0.62       0.51012552 0.38455462 0.29362103 0.22013694]\n",
            " [0.63       0.52061796 0.39315611 0.29990453 0.22431694]\n",
            " [0.64       0.53118646 0.40176684 0.30624282 0.22855373]\n",
            " [0.65       0.54175496 0.41037759 0.31258118 0.23279047]\n",
            " [0.66       0.55227476 0.41895509 0.3189047  0.23704123]\n",
            " [0.67       0.56167936 0.42677075 0.32488781 0.24161232]\n",
            " [0.68       0.5710839  0.43458641 0.33087093 0.24618334]\n",
            " [0.69       0.58048844 0.44240201 0.33685401 0.25075436]\n",
            " [0.7        0.58987081 0.45036834 0.34290364 0.25537455]\n",
            " [0.71       0.59925276 0.45833802 0.34895474 0.25999582]\n",
            " [0.72       0.60863459 0.46630758 0.3550058  0.26461703]\n",
            " [0.73       0.61801648 0.4742772  0.36105689 0.26923829]\n",
            " [0.74       0.62754989 0.48237938 0.36720634 0.27393419]\n",
            " [0.75       0.63719583 0.49058002 0.37342885 0.27868551]\n",
            " [0.76       0.64684176 0.49878061 0.37965137 0.28343686]\n",
            " [0.77       0.6564877  0.50698125 0.38587391 0.28818822]\n",
            " [0.78       0.66613364 0.51518178 0.39209634 0.29293954]\n",
            " [0.79       0.6757797  0.52338248 0.39831895 0.29769093]\n",
            " [0.8        0.68542558 0.53158307 0.40454143 0.30244225]\n",
            " [0.81       0.69507158 0.53978372 0.41076398 0.30719364]\n",
            " [0.82       0.70471752 0.54798436 0.41698647 0.31194496]\n",
            " [0.83       0.71436334 0.55618495 0.42320901 0.31669629]\n",
            " [0.84       0.72400939 0.56438559 0.4294315  0.32144767]\n",
            " [0.85       0.73365545 0.5725863  0.43565416 0.32619905]\n",
            " [0.86       0.74330139 0.58078688 0.44187659 0.33095038]\n",
            " [0.87       0.75294721 0.58898753 0.44809914 0.3357017 ]\n",
            " [0.88       0.76259327 0.59718812 0.45432168 0.34045306]\n",
            " [0.89       0.77243841 0.60556042 0.46067411 0.34530228]\n",
            " [0.9        0.78248703 0.61410803 0.46715933 0.35025156]\n",
            " [0.91       0.79253578 0.62265575 0.47364461 0.35520089]\n",
            " [0.92       0.80258453 0.63120341 0.48012996 0.36015019]\n",
            " [0.93       0.81263328 0.63975108 0.48661518 0.36509946]\n",
            " [0.94       0.82268202 0.6482988  0.49310046 0.37004876]\n",
            " [0.95       0.83273077 0.65684652 0.49958575 0.37499809]\n",
            " [0.96       0.84302807 0.66561037 0.5062452  0.3800686 ]\n",
            " [0.97       0.85351968 0.6745494  0.51304817 0.38523343]\n",
            " [0.98       0.86401141 0.68348861 0.51985121 0.39039835]\n",
            " [0.99       0.87450302 0.69242764 0.52665418 0.39556316]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}